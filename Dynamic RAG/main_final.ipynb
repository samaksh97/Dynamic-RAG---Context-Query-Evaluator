{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e66744-d656-4171-bcf7-cc8f7538799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-01-29 00:54:31.285801: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-29 00:54:31.291936: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-29 00:54:31.343287: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-29 00:54:31.343338: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-29 00:54:31.343388: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 00:54:31.354832: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-29 00:54:31.356909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 00:54:37.121199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from prompts_constants import *  # Ensure constants like USERNAME, PASSWORD, PGVECTOR_CONNECTION_STRING are imported\n",
    "from evaluate_context import TextProcessor, SemanticSimilarity\n",
    "from embeddings import SentenceTransformerEmbeddingWrapper\n",
    "from llm import LangchainDSXLLM  # Import the LLM setup from llm.py\n",
    "\n",
    "\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to initialize the vector store\n",
    "def initialize_vector_store():\n",
    "    embedding_model = SentenceTransformerEmbeddingWrapper('sentence-transformers/all-MiniLM-L12-v2')\n",
    "    store = PGVector(\n",
    "        embeddings=embedding_model,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection=PGVECTOR_CONNECTION_STRING,\n",
    "        use_jsonb=True\n",
    "    )\n",
    "    return store\n",
    "\n",
    "\n",
    "\n",
    "def create_dynamic_rag_chain(k, llm, store, prompt_template):\n",
    "    retriever = store.as_retriever(search_kwargs={\"k\": k})\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={\n",
    "            \"prompt\": prompt_template,\n",
    "            \"document_variable_name\": \"s\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "def fact_check_dataframe(df, context_col, claim_col, api_key):\n",
    "    def check_factuality(context, claim):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.bespokelabs.ai/v0/argus/factcheck\",\n",
    "                json={\n",
    "                    \"contexts\": [context],  # Ground truth from context column\n",
    "                    \"claim\": claim          # Claim from claim column\n",
    "                },\n",
    "                headers={\"api_key\": api_key}\n",
    "            )\n",
    "            # Pause to avoid overloading the API\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Parse and extract the 'claim_supported_by_contexts' value if it exists\n",
    "            result = response.json()\n",
    "            return result.get('claim_supported_by_contexts', [None])[0]  # Extracting first element or None if not found\n",
    "        except Exception as e:\n",
    "            return None  # Return None if there is an error\n",
    "\n",
    "    # Apply the factuality check to each row in the DataFrame\n",
    "    df['factuality_score'] = df.apply(\n",
    "        lambda row: check_factuality(row[context_col], row[claim_col]), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function to evaluate the final context\n",
    "def evaluate_final_context(query, final_context, text_processor, semantic_similarity, api_key=None):\n",
    "    \"\"\"\n",
    "    Evaluates the final context with the query using various metrics and performs factuality checks.\n",
    "    \"\"\"\n",
    "    # Compute semantic similarity score\n",
    "    semantic_scores = semantic_similarity.compute_similarity(query, [final_context])\n",
    "    semantic_score = semantic_scores[0]\n",
    "\n",
    "    # Compute keyword matching score\n",
    "    query_keywords = text_processor.preprocess(query, 'lemma')\n",
    "    keyword_score = text_processor.keyword_match_score(query_keywords, final_context, 'lemma')\n",
    "\n",
    "    # Context length penalty\n",
    "    context_length_penalty = 1.0 - min(len(final_context.split()) / 500, 1.0)\n",
    "\n",
    "    # Aggregate confidence score\n",
    "    confidence_score = (\n",
    "        0.6 * semantic_score +\n",
    "        0.3 * keyword_score -\n",
    "        0.1 * context_length_penalty\n",
    "    )\n",
    "\n",
    "    # Fact-checking (optional, requires API key)\n",
    "    factuality_score = None\n",
    "    if api_key:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.bespokelabs.ai/v0/argus/factcheck\",\n",
    "                json={\n",
    "                    \"contexts\": [final_context],  # Ground truth from context\n",
    "                    \"claim\": query               # Claim from query\n",
    "                },\n",
    "                headers={\"api_key\": api_key}\n",
    "            )\n",
    "            # Parse response for factuality score\n",
    "            result = response.json()\n",
    "            factuality_score = result.get('claim_supported_by_contexts', [None])[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Fact-checking failed: {e}\")\n",
    "            factuality_score = None\n",
    "\n",
    "    # Aggregate results\n",
    "    evaluation_results = {\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"keyword_score\": keyword_score,\n",
    "        \"context_length_penalty\": context_length_penalty,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"factuality_score\": factuality_score  # Include factuality score if computed\n",
    "    }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "# Function to generate a summary using Azure LLM\n",
    "def generate_summary_with_llm(query, context_evaluation, best_chunks, azure_llm):\n",
    "    top_chunks_summary = \"\\n\".join(\n",
    "        [\n",
    "            f\"Chunk {i+1}: Semantic Score = {chunk['semantic_score']:.4f}, \"\n",
    "            f\"Keyword Score = {chunk['keyword_score']:.4f}\\n\"\n",
    "            f\"Content: {chunk['chunk'][:200]}...\"\n",
    "            for i, chunk in enumerate(best_chunks)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant summarizing evaluation metrics for a chatbot query.\\n\\n\"\n",
    "        f\"User Query: '{query}'\\n\\n\"\n",
    "        f\"The chatbot selected the following top chunks based on their relevance:\\n\"\n",
    "        f\"{top_chunks_summary}\\n\\n\"\n",
    "        f\"The evaluation metrics for the final context were as follows:\\n\"\n",
    "        f\"- Semantic Similarity Score: {context_evaluation['semantic_score']:.4f}\\n\"\n",
    "        f\"- Keyword Matching Score: {context_evaluation['keyword_score']:.4f}\\n\"\n",
    "        f\"- Confidence Score: {context_evaluation['confidence_score']:.4f}\\n\"\n",
    "        f\"- Factuality Score: {context_evaluation['factuality_score']}\\n\\n\"\n",
    "        f\"Please generate a concise and user-friendly summary explaining these metrics and how they relate to the query and selected chunks.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        summary = azure_llm._call(prompt)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Error during LLM call: {str(e)}\"\n",
    "# Function to handle queries dynamically with RAG\n",
    "\n",
    "# Function to handle queries dynamically with RAG\n",
    "def answer_query_with_dynamic_rag(query, chat_history, store, azure_llm, prompt, text_processor, semantic_similarity, api_key):\n",
    "    initial_rag_chain = create_dynamic_rag_chain(3, azure_llm, store, prompt)\n",
    "    initial_result = initial_rag_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "    retrieved_chunks = [doc.page_content for doc in initial_result[\"source_documents\"]]\n",
    "\n",
    "    print(\"Computing semantic similarity...\")\n",
    "    semantic_scores = semantic_similarity.compute_similarity(query, retrieved_chunks)\n",
    "\n",
    "    print(\"Computing keyword matching...\")\n",
    "    query_keywords = text_processor.preprocess(query, 'lemma')\n",
    "    keyword_scores = [\n",
    "        text_processor.keyword_match_score(query_keywords, chunk, 'lemma') for chunk in retrieved_chunks\n",
    "    ]\n",
    "\n",
    "    chunk_scores = [\n",
    "        {\n",
    "            \"chunk\": chunk,\n",
    "            \"semantic_score\": semantic_scores[i],\n",
    "            \"keyword_score\": keyword_scores[i],\n",
    "        }\n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ]\n",
    "\n",
    "    for chunk_score in chunk_scores:\n",
    "        chunk_score[\"combined_score\"] = (\n",
    "            0.5 * chunk_score[\"semantic_score\"] + 0.5 * chunk_score[\"keyword_score\"]\n",
    "        )\n",
    "    sorted_chunks = sorted(chunk_scores, key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "\n",
    "    best_n = max(range(1, 4), key=lambda n: sum(\n",
    "        sorted_chunks[i][\"combined_score\"] for i in range(n)\n",
    "    ) / n)\n",
    "    print(f\"Decided to use top-{best_n} chunks based on scores.\")\n",
    "\n",
    "    dynamic_rag_chain = create_dynamic_rag_chain(best_n, azure_llm, store, prompt)\n",
    "    final_result = dynamic_rag_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    answer = final_result[\"answer\"]\n",
    "    sources = final_result[\"source_documents\"]\n",
    "\n",
    "    best_chunks = sorted_chunks[:best_n]\n",
    "    final_context = \" \".join(chunk[\"chunk\"] for chunk in best_chunks)\n",
    "    context_evaluation = evaluate_final_context(query, final_context, text_processor, semantic_similarity, api_key=api_key)\n",
    "\n",
    "    summary = generate_summary_with_llm(query, context_evaluation, best_chunks, azure_llm)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": [doc.page_content[:200] for doc in sources],\n",
    "        \"selected_k\": best_n,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"context_evaluation\": context_evaluation,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d791aa-2192-490f-85d2-a1469699a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/dell/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing semantic similarity...\n",
      "Computing keyword matching...\n",
      "Decided to use top-1 chunks based on scores.\n",
      "Fact-checking failed: name 'requests' is not defined\n",
      "\n",
      "Final Answer:\n",
      "The capital of France is Paris.\n",
      "\n",
      "Summary:\n",
      "Based on the user query, which asked for the capital of France, the chatbot provided a chunk of text that was not relevant to the query. The metrics used to evaluate this response provide insights into its quality. The semantic similarity score indicates how closely the provided chunk matches the user's query, and in this case, it was very low at 0.1565. Additionally, the keyword matching score, which determines how well the chunk includes relevant keywords, was 0.5000, suggesting only partial match. The confidence score, indicating the chatbot's certainty in its response, was 0.1777, which is relatively low. Lastly, there was no factuality score provided, meaning the chunk did not contain accurate information related to the query. Overall, the metrics indicate that the chatbot's response was not satisfactory, as it failed to provide the correct answer or relevant information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution workflow\n",
    "if __name__ == \"__main__\":\n",
    "    text_processor = TextProcessor()\n",
    "    semantic_similarity = SemanticSimilarity()\n",
    "\n",
    "    store = initialize_vector_store()\n",
    "    llm = LangchainDSXLLM()\n",
    "\n",
    "    query = \"What is the capital of France?\"\n",
    "    chat_history = []\n",
    "\n",
    "    # Wrap FIRST_PROMPT in a PromptTemplate\n",
    "    FIRST_PROMPT_TEMPLATE = PromptTemplate(\n",
    "        template=FIRST_PROMPT,\n",
    "        input_variables=[\"s\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Define a query\n",
    "    query = \"What is the capital of France?\"\n",
    "    chat_history = []\n",
    "    bespoke_api_key = \"\"  # Replace with your actual API key\n",
    "    \n",
    "    # Use RAG chain\n",
    "    result = answer_query_with_dynamic_rag(\n",
    "        query=query,\n",
    "        chat_history=chat_history,\n",
    "        store=store,\n",
    "        azure_llm=llm,\n",
    "        prompt=FIRST_PROMPT_TEMPLATE,\n",
    "        text_processor=text_processor,\n",
    "        semantic_similarity=semantic_similarity,\n",
    "        api_key=bespoke_api_key\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\nSummary:\")\n",
    "    print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e47e9b-3088-4158-bedc-06201bba36b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact-checking failed: name 'requests' is not defined\n",
      "\n",
      "Final Answer:\n",
      "The reason behind Justin Trudeau announcing his resignation as Canada's prime minister is due to internal battles, declining party support, growing political polarization, economic challenges, high inflation, and tensions with the incoming U.S. administration of President-elect Donald Trump, particularly over proposed tariffs.\n",
      "\n",
      "Summary:\n",
      "The evaluation metrics for the chatbot's response to the query \"Why did Justin Trudeau announce his resignation as Canada's prime minister?\" are as follows:\n",
      "\n",
      "1. Semantic Similarity Score: This score indicates the degree of similarity between the selected chunk and the user's query. In this case, the score is 0.7416, suggesting that the chunk is semantically relevant to the query.\n",
      "\n",
      "2. Keyword Matching Score: This score measures how well the keywords in the selected chunk match the keywords in the user's query. The score is 0.8571, indicating a high level of keyword similarity.\n",
      "\n",
      "3. Confidence Score: The confidence score represents the bot's level of certainty in the selected chunk's relevance to the query. In this case, the score is 0.6225, suggesting a moderate level of confidence.\n",
      "\n",
      "4. Factuality Score: Unfortunately, the factuality score is not provided for this evaluation. This score usually indicates the accuracy or truthfulness of the selected chunk.\n",
      "\n",
      "In summary, the selected chunk has a high semantic similarity and keyword matching score, indicating its relevance to the query about Justin Trudeau's resignation. The confidence score suggests moderate certainty, but the factuality of the information is not available.\n",
      "\n",
      "Sources:\n",
      "Canadian Prime Minister Justin Trudeau announced his resignation as Liberal Party leader and prime minister, citing internal battles, declining party support, and growing political polarization. Trude\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main execution workflow\n",
    "if __name__ == \"__main__\":\n",
    "    text_processor = TextProcessor()\n",
    "    semantic_similarity = SemanticSimilarity()\n",
    "\n",
    "    store = initialize_vector_store()\n",
    "    llm = LangchainDSXLLM()\n",
    "\n",
    "    # Wrap FIRST_PROMPT in a PromptTemplate\n",
    "    FIRST_PROMPT_TEMPLATE = PromptTemplate(\n",
    "        template=FIRST_PROMPT,\n",
    "        input_variables=[\"s\", \"question\"]\n",
    "    )\n",
    "\n",
    "    bespoke_api_key = \"bespoke-0e6e9818ac3f0cf5fcac9ebc910c52f470181cb885003a5444b4e9b330fc3e19\"  # Replace with your actual API key\n",
    "\n",
    "    # Start an interactive session for multiple queries\n",
    "    print(\"Enter your queries below (type 'exit' to stop):\")\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        query = input(\"Your query: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Exiting the query session.\")\n",
    "            break\n",
    "\n",
    "        # Process the query\n",
    "        try:\n",
    "            result = answer_query_with_dynamic_rag(\n",
    "                query=query,\n",
    "                chat_history=chat_history,\n",
    "                store=store,\n",
    "                azure_llm=llm,\n",
    "                prompt=FIRST_PROMPT_TEMPLATE,\n",
    "                text_processor=text_processor,\n",
    "                semantic_similarity=semantic_similarity,\n",
    "                api_key=bespoke_api_key\n",
    "            )\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\nFinal Answer:\")\n",
    "            print(result[\"answer\"])\n",
    "            print(\"\\nSummary:\")\n",
    "            print(result[\"summary\"])\n",
    "            print(\"\\nSources:\")\n",
    "            for source in result[\"sources\"]:\n",
    "                print(source)\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "            # Append the query to the chat history for context in the next iterations\n",
    "            chat_history.append((query, result[\"answer\"]))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing the query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a438d-7172-499d-a6c4-34416c827282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(env_try_sg) Python",
   "language": "python",
   "name": "conda-env-env_try_sg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
